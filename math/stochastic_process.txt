1. Definition of Stochastic Process

	A stochastic process is a collection of random variables indexed by time.
	Discrete-time: X_0, X_1, X_2, ...
	Continuous-time: X(t), where t is continuous

	Alternate View:

	A probability distribution over paths (entire sequences of values), more suitable for formal mathematics.

2. Examples of Stochastic Processes

	f(t) = t: Deterministic, no randomness.
	f(t) = t or -t for all t with probability 0.5 each: Entire path is random but consistent once chosen.
	f(t) = ±t for each t independently: Maximum randomness; values at each time are independently chosen.

3. Motivation and Use

	Real-world applications (e.g., stock prices): Observe past to infer the future.
	Goal: Use probability distribution to reason about the future given the past.

4. Core Questions in Stochastic Processes

	Dependencies: How does the past affect the future?
	Long-term Behavior: Will the process converge or stabilize?
	Boundary Events: What is the probability of extreme or rare events?

5. Simple Random Walk
	
	https://youtu.be/stgYW6M5o4k?si=xTQlczw5BAxsSfqk
	Definition:
		Let Y_i be i.i.d. random variables taking values ±1 with equal probability.
		Define X_0 = 0, and X_t = sum(Y_1 to Y_t)

	Key Properties:

		E[X_t] = 0
		Independent Increments
		Stationary Increments

	Central Limit Theorem Insight:

		X_t / sqrt(t) ~ N(0,1) as t -> ∞

	Random walk stays within bounds ±√t most of the time.

	Application Example:
		Gambler’s ruin problem: https://youtu.be/f_MUjkr0yu4?si=yZbN53q0JmWdLlpX
		Compute probability of reaching a target value before hitting a lower threshold.

6. Markov Chains
	https://youtu.be/prZMpThbU3E?si=tRFj3x0uJZtQPNa4
	Definition:
		A stochastic process is a Markov Chain if:
			P(X_{t+1} | X_0, ..., X_t) = P(X_{t+1} | X_t)

	Characteristics:

		Memoryless: Only current state matters for future transitions.
			Useful for modeling systems where history is irrelevant after current state is known.
		
		Transition Probability Matrix:
			Matrix P where P_ij = P(X_{t+1} = j | X_t = i) ... (next P depends on current P)
			Row sums = 1
			Powers of P (P^n) give n-step transition probabilities
			
		
		Stationary Distribution:
			As t → ∞, P(Xₜ = i) converges to stationary distribution π.
			π is an eigenvector of transition matrix P such that Pπ = π
			Describes long-run behavior of the Markov chain
			Guaranteed to exist under conditions (e.g., all entries of P positive)
			Corresponds to eigenvalue λ = 1

7. Martingales
	https://youtu.be/zTsRGQj6VT4?si=Kg5jm9JvE9eqhwq9
	Definition:

		A stochastic process {X_t} is a martingale if:
			E[X_{t+1} | X_0, ..., X_t] = X_t

	Intuition:

		Models a fair game; expected value stays the same over time.
		
		Examples:
			Simple random walk → martingale
			Casino games → not martingales (expected loss)
			Custom example: X_k = product of Y_i (Y_i = 2 w.p. 1/3, 1/2 w.p. 2/3) → martingale

8. Optional Stopping Theorem

	Theorem:
	
		If {X_t} is a martingale and τ is a stopping time bounded by T: 
			
			τ is almost surely finite (you'll eventually stop), 
			τ≤T for some constant T (you don't play forever),
		Then:
			E[X_τ] = E[X_0]
			
			In simple terms: No matter what strategy you use to stop, you can't win or lose in expectation in a fair game.

	Definition of Stopping Time:
		A random variable τ is a stopping time if the decision to stop by time t depends only on information up to t.

	Application:
		In a fair game (martingale), no strategy can alter the expected outcome.
		Used to prove probability of winning/losing scenarios like gambler’s ruin.
		
	Example:
		You're playing a fair coin toss game: 
			Heads: you win $1 
			Tails: you lose $1 
		You start with $0. 
		You stop playing after you get 10 wins (i.e., 10 heads total), no matter how many losses you've had 
		
		Why this fits OST: 
			{X_t} is a martingale (fair coin toss game) 
			τ is a stopping time: you stop after reaching 10 wins, which depends only on your past results. 
			τ is finite: eventually, you’ll get 10 heads. 
		
		Now From OST: 
			𝐸 [ 𝑋 𝜏 ] = 𝐸 [ 𝑋 0 ] = 0 
			
			So even though you’re stopping after 10 wins, your expected final balance is still $0.
			
			Why? 
			Because while you’re guaranteed to win $10 from the heads, you also have to endure a random number of losses before reaching 10 wins. 
			On average, your total losses cancel out the gains, and the expectation stays 0. 

			Intuition:
			Even if you stop at a moment that seems favorable, the overall expected outcome cannot beat the game if the game is fair and you stop fairly (i.e., without peeking into the future).
		
		Why OST didnt work in Martingale betting strategy:
			Because OST only applies under certain conditions, and this strategy violates one of them. 
			
			Violation: Unbounded Stopping Time 
			The Martingale strategy does not guarantee that you'll stop within a finite amount of time. 
			The stopping time τ, defined as the time you first win, could in theory be infinite — if you just keep losing forever (low probability, but nonzero).

Summary:

Stochastic Process: Random variables over time
Simple Random Walk: Fundamental model with i.i.d. steps
Markov Chain: Memoryless stochastic process
Martingale: Models fair games, expectation stays constant
Optional Stopping: No strategy can beat a fair game in expectation

